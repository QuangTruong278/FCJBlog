[{"uri":"https://quangtruong278.github.io/FCJBlog/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Van Quang Truong\nPhone Number: 0765243702\nEmail: truongvqse182458@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial intelligence\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 28/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will find the weekly breakdown of my internship at First Cloud Journey. Over the course of 12 weeks (from September 8th to November 28th), I followed a structured roadmap to master Cloud Computing and DevOps practices on AWS.\nThe worklog below details the progression from basic AWS concepts to advanced topics like Serverless, IaC, and CI/CD:\nWeek 1: Getting familiar with AWS and Basic Services (EC2, CLI)\nWeek 2: Deep dive into Networking (VPC) \u0026amp; Storage (S3)\nWeek 3: Managed Databases (RDS) \u0026amp; Web Server Integration\nWeek 4: High Availability with Load Balancing (ALB) \u0026amp; Auto Scaling (ASG)\nWeek 5: Identity and Access Management (IAM) \u0026amp; Security Best Practices\nWeek 6: Email Integration with Amazon SES (Simple Email Service)\nWeek 7: Infrastructure as Code (IaC) with Terraform\nWeek 8: Building CI/CD Pipelines with AWS Developer Tools\nWeek 9: Containerization with Docker \u0026amp; Amazon ECR\nWeek 10: Serverless Computing with AWS Lambda \u0026amp; API Gateway\nWeek 11: System Monitoring \u0026amp; Logging with Amazon CloudWatch\nWeek 12: Final Project Review, Optimization \u0026amp; Resource Cleanup\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS - SSH connection methods to EC2 - Learn about Elastic IP 11/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups: Compute, Storage, Networking, Database. Successfully created and configured an AWS Free Tier account. Became familiar with the AWS Management Console. Installed and configured AWS CLI (Access Key, Secret Key, Region). Used AWS CLI to perform basic operations (Check account, list regions, manage key pairs). Acquired the ability to connect between the web interface and CLI. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.3-compute/5.3.1-ec2/","title":"Amazon EC2","tags":[],"description":"","content":"Initialize EC2 Instance Steps 1. Create Security Group Create a Security Group allowing HTTP (80), HTTPS (443), SSH (22), and Custom TCP (8080 - Spring Boot port). 2. Launch Instance Access EC2 Dashboard -\u0026gt; Launch Instances.\nName: Auction-Backend.\nSelect OS: Amazon Linux 2023 or Ubuntu. Select Instance Type: t3.medium (as proposed).\nSelect Key Pair (create new if not exists).\nNetwork settings: Select VPC, Public Subnet, and the created Security Group. Configure storage (default 8GB or increase if needed).\nAdvanced details:\nIAM instance profile: Select Auction-EC2-Role. Click Launch instance.\n3. Assign Elastic IP (Optional) If you want a fixed Public IP.\nGo to Elastic IPs -\u0026gt; Allocate Elastic IP address. Select the created IP -\u0026gt; Associate Elastic IP address. Select the created Instance. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.2-database-storage/5.2.1-rds/","title":"Amazon RDS","tags":[],"description":"","content":"Initialize Amazon RDS (MySQL) We will use Amazon RDS MySQL to store the main data of the system.\nSteps 1. Create Security Group for RDS First, we need to create a Security Group allowing connection on port 3306 from EC2. 2. Create Subnet Group Go to RDS Dashboard -\u0026gt; Subnet groups -\u0026gt; Create DB subnet group. Select the VPC and the created Private Subnets. 3. Create Database Select Databases -\u0026gt; Create database.\nSelect Standard create -\u0026gt; MySQL.\nSelect version (Engine Version). Select Free tier (if using a new account) or Dev/Test. Set DB instance identifier, Master username and password. Select Instance class (e.g., db.t3.micro).\nConfigure Storage. Configure Connectivity: Select VPC, Subnet Group, and the created Security Group. Configure authentication (Password authentication). Click Create database. "},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.4-distribution/5.4.1-ssl-certificate/","title":"Certificate Manager","tags":[],"description":"","content":"AWS Certificate Manager (ACM) To use HTTPS, we need an SSL certificate.\nSteps Access ACM Console.\nSelect Request a certificate.\nSelect Request a public certificate. Enter Domain name (e.g., *.example.com). Select DNS validation.\nClick Request.\nAfter requesting, click on the certificate ID, select Create records in Route 53 for automatic validation. Wait for status to change to Issued.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.1-preparation/5.1.1-vpc/","title":"Create VPC","tags":[],"description":"","content":"Initialize Virtual Private Cloud (VPC) The system will run inside a Virtual Private Cloud (VPC) to ensure security and resource isolation. We will create a VPC with Public Subnets (for Load Balancers) and Private Subnets (for EC2, RDS).\nSteps Access the AWS Console and search for the VPC service. Select Create VPC. Choose VPC and more configuration to quickly create a VPC along with subnets and Route Tables. Fill in the information: Name tag: Auction-VPC IPv4 CIDR block: 10.0.0.0/16 Number of Availability Zones (AZs): 2 (to ensure high availability) Number of public subnets: 2 Number of private subnets: 2 NAT gateways: None (or 1 per AZ if EC2 in private subnet needs internet access to download packages, but to save costs in this lab, you can choose None or 1). Click Create VPC. Wait for the initialization process to complete. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.5-deploy/5.5.2-gitlab/","title":"GitLab CI","tags":[],"description":"","content":"Configure GitLab CI Steps 1. Prepare Variables Go to Settings -\u0026gt; CI/CD -\u0026gt; Variables on the GitLab Repository. Add necessary variables:\nEC2_IP SSH_PRIVATE_KEY 2. Configuration file .gitlab-ci.yml Create a .gitlab-ci.yml file at the root of the project. The workflow includes:\nBuild: Build JAR file (Spring Boot) or Docker Image. Deploy: Copy file to EC2 and restart service. Example of a successful pipeline: "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.1-preparation/","title":"Preparation","tags":[],"description":"","content":"Environment Preparation Before installing the main services, we need to prepare the Networking layer and Access Rights (IAM) for the resources.\nContent VPC: Create a Virtual Private Cloud to isolate the network for the system. IAM: Create necessary Roles for EC2 to access S3, Rekognition, and Textract. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.5-deploy/5.5.1-setup-ec2/","title":"Setup EC2 Instance Environment","tags":[],"description":"","content":"Environment Setup for EC2 Server In this section, we will assign an IAM Role to the EC2 instance and install necessary software such as Java and MariaDB to prepare for application deployment.\n1. Assign IAM role to EC2 To allow the EC2 instance to access other AWS services (e.g., Session Manager for connection without opening SSH ports, or accessing S3, RDS), we need to assign an appropriate IAM Role.\nAccess EC2 Dashboard, select the Instance you just created. Select Actions -\u0026gt; Security -\u0026gt; Modify IAM role. Select the IAM Role created in previous steps (e.g., EC2RoleForSSM) and click Update IAM role. 2. Environment Setup Connect to the EC2 Instance (using Session Manager or SSH). Then perform the following steps sequentially:\n2.1. Update System Run the following command to update to the latest software packages:\nsudo dnf update -y 2.2. Install Java Our application runs on the Java platform, so installing the Java Development Kit (JDK) is required. Here we use Amazon Corretto 21 (headless version is more suitable for command-line interfaces without graphics).\nsudo dnf install java-21-amazon-corretto-headless -y Check Java version after installation:\njava -version 2.3. Install MariaDB Client and Initialize Database Install MariaDB client to connect and interact with RDS.\nsudo dnf install mariadb105 -y Connect to the created RDS database instance. Replace \u0026lt;rds-endpoint\u0026gt;, \u0026lt;username\u0026gt; with your actual information:\nmysql -h \u0026lt;rds-endpoint\u0026gt; -u \u0026lt;username\u0026gt; -p After successfully entering the password, create the database for the application:\nCREATE DATABASE tickets; SHOW DATABASES; 2.4. Setup Service for Auto-starting Java Springboot Application Run the following command to create a service file\nsudo nano /etc/systemd/system/\u0026lt;service-name\u0026gt;.service Enter the service file content, configure environment variables for the application\nUse key combination Ctrl + O, Enter and Ctrl + X to save and exit.\nUse the following commands to apply changes\nsudo systemctl daemon-reload sudo systemctl restart \u0026lt;service-name\u0026gt; Check status using command status Use command enable so the service automatically runs every time the EC2 instance starts\nsudo systemctl enable \u0026lt;service-name\u0026gt; Command to set timezone to synchronize (in case Java application needs to match Vietnam time)\nsudo timedatectl set-timezone Asia/Ho_Chi_Minh Check result with command\ndate "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deep dive into AWS Networking (VPC, Subnet, Route Table, Internet Gateway). Understand and practice AWS Storage Service (S3) and its features (Versioning, Lifecycle, Hosting). Deploy a static website on S3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn VPC Architecture: CIDR, Subnets (Public vs Private), Internet Gateway.\n- Practice: Create a custom VPC with 1 Public Subnet and 1 Private Subnet. 15/09/2025 15/09/2025 https://docs.aws.amazon.com/vpc/ 3 - Configure Route Tables and Security Groups (Inbound/Outbound rules).\n- Launch an EC2 inside the Public Subnet and try to SSH.\n- Understand NAT Gateway basics. 16/09/2025 16/09/2025 https://docs.aws.amazon.com/vpc/ 4 - Learn Amazon S3 concepts: Buckets, Objects, Classes (Standard, IA, Glacier).\n- Practice: Create S3 Buckets via Console and CLI. 17/09/2025 17/09/2025 https://docs.aws.amazon.com/s3/ 5 - Advanced S3 Features: Enable Versioning, Configure Lifecycle Rules.\n- Learn about Bucket Policies and ACLs. 18/09/2025 18/09/2025 https://docs.aws.amazon.com/s3/ 6 - Mini-Project: Host a Static Website on Amazon S3.\n- Upload HTML/CSS files.\n- Configure \u0026ldquo;Static website hosting\u0026rdquo; and make objects public. 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Successfully designed and created a custom Virtual Private Cloud (VPC) with correct network segmentation (Public/Private subnets). Configured Security Groups to strictly control traffic (e.g., allow SSH only from specific IP). Mastered Amazon S3 fundamentals: Created buckets and managed objects using both Console and CLI. Implemented data protection using Versioning. Optimized costs using Lifecycle policies (moving data to Glacier). Successfully hosted a static website using S3 without provisioning any servers. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.2-database-storage/5.2.2-elasticache/","title":"Amazon ElastiCache","tags":[],"description":"","content":"Initialize Amazon ElastiCache (Redis) Redis helps cache frequent queries and store user sessions.\nSteps 1. Create Subnet Group Go to ElastiCache Dashboard -\u0026gt; Subnet groups -\u0026gt; Create subnet group. 2. Create Security Group Create a Security Group allowing port 6379 from EC2. 3. Create Redis Cluster Select Redis OSS caches -\u0026gt; Create cache.\nSelect Configure and create a new cluster.\nSelect Cluster mode disabled (for simplicity and cost savings). Configure Redis info. Configure Node type, e.g., cache.t3.micro. Select Subnet group. Select Security Group. Click Create.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.4-distribution/5.4.2-alb/","title":"Application Load Balancer","tags":[],"description":"","content":"Application Load Balancer (ALB) ALB will distribute traffic to EC2 instances and handle SSL termination.\nSteps 1. Create Security Group for ALB Allow HTTP (80) and HTTPS (443) from 0.0.0.0/0. 2. Create Target Group Create a Target Group of type Instances. Protocol HTTP/8080 (Backend port). Register EC2 instances into the Target Group. 3. Create Load Balancer Go to Load Balancers -\u0026gt; Create load balancer.\nSelect Application Load Balancer.\nSelect VPC and Public Subnets.\nSelect the created Security Group.\nConfigure Listeners:\nHTTP:80 -\u0026gt; Redirect to HTTPS (recommended). HTTPS:443 -\u0026gt; Forward to Target Group. In the HTTPS listener section, select the created ACM Certificate. Click Create load balancer.\nAfter creating the Load Balancer, you can reconfigure the security group for the EC2 instance to only accept inbound rules from the Load Balancer. "},{"uri":"https://quangtruong278.github.io/FCJBlog/4-eventparticipated/4.1-event1/","title":"AWS Event Report: GenAI &amp; Data Strategies","tags":[],"description":"","content":"Summary Report: “AWS GenAI \u0026amp; Data Revolution” Event Objectives Understand the strategic roadmap for Generative AI adoption on AWS. Learn how to build a unified data foundation to support AI and analytics workloads. Explore the AI-Driven Development Lifecycle (AI-DLC) and the future of software implementation. Master best practices for securing GenAI applications and leveraging AI Agents. Speakers Keynote \u0026amp; Panelists:\nEric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles – VP, General Manager APJ, AWS Dr. Jens Lottner – CEO, Techcombank Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jeff Johnson – Managing Director, ASEAN, AWS (Moderator) Technical Track Speakers:\nJun Kai Loke – AI/ML Specialist SA, AWS Kien Nguyen – Solutions Architect, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Highlights Navigating the GenAI Revolution (Executive Panel) Leadership Strategy: Discussion on how executives can steer organizations through rapid GenAI advancements. Culture of Innovation: Insights from ELSA Corp, Nexttech Group, and TymeX on aligning AI initiatives with business objectives and managing organizational change. Building a Unified Data Foundation Infrastructure: Strategies for constructing a scalable data foundation on AWS tailored for AI/Analytics. Components: Deep dive into data ingestion, storage, processing, and governance. Goal: Ensuring organizations can effectively manage data to fuel advanced AI initiatives. AI-Driven Development Lifecycle (AI-DLC) Shift in Paradigm: Moving from \u0026ldquo;AI as an assistant\u0026rdquo; to \u0026ldquo;AI as a central collaborator.\u0026rdquo; Integration: Integrating AI-powered execution with human oversight to drastically improve software development speed and quality. Securing Generative AI Applications Multi-layer Security: Addressing challenges at the infrastructure, model, and application layers. Best Practices: Implementing zero-trust architecture, encryption, continuous monitoring, and fine-grained access controls to safeguard data confidentiality. Beyond Automation: AI Agents Productivity Multipliers: AI agents are evolving from simple tools to intelligent partners. Autonomy: These agents learn, adapt, and execute complex tasks autonomously, transforming operations from manual drudgery to exponential efficiency. Key Takeaways Strategic Insight Data is Key: A robust, unified data foundation is the prerequisite for any successful GenAI strategy. Security First: Security cannot be an afterthought; it must be embedded in the model and infrastructure layers (Zero Trust). Technical Evolution AI-DLC: The software development lifecycle is changing fundamentally. We need to adapt to working with AI as a partner, not just a tool. Agentic Workflow: The future belongs to AI Agents that can reason and act, not just generate text. Applying to Work Review Data Strategy: Assess current data infrastructure to ensure it is unified and ready for AI workloads (referencing AWS best practices). Experiment with AI Agents: Explore building simple AI Agents for repetitive operational tasks to boost team productivity. Security Audit: Review current security measures for any AI/ML models currently in testing or production. Adopt AI-DLC: Encourage the team to integrate AI coding assistants deeper into the dev workflow to improve speed. Event Experience Participating in the \u0026ldquo;Gen AI and Data\u0026rdquo; track provided a clear roadmap for the future of technology on AWS.\nInsights from Industry Leaders The panel discussion with leaders from Techcombank and ELSA Corp was particularly inspiring, showing that GenAI is not just a technical trend but a core business driver that requires a cultural shift.\nDeep Dive into Tech I was impressed by the session on AI-Driven Development Lifecycle (AI-DLC). It changed my perspective on how software will be built in the future—where AI is a collaborator, not just a helper.\nFuture-Proofing The session on AI Agents by Michael Armentano opened my eyes to the difference between simple automation and intelligent agents. It motivated me to research more about how to build autonomous agents on AWS.\nSome event photos Overall, the event bridged the gap between high-level AI strategy and the practical technical steps needed to implement it (Data, Security, and Agents).\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.1-preparation/5.1.2-iam/","title":"Create IAM Role","tags":[],"description":"","content":"Create IAM Role for EC2 EC2 needs access to S3 to retrieve code/images, and permissions to call Rekognition and Textract APIs. Instead of storing Access Keys in the code, we will use an IAM Role attached to the EC2 instance.\nSteps Access IAM Dashboard. Select Roles -\u0026gt; Create role. In the Trusted entity type step, select AWS service. In Use case, select EC2. Click Next. In the Add permissions step, search and select the following Policies: AmazonS3FullAccess (Or a policy limited to the project bucket only). AmazonRekognitionFullAccess. AmazonTextractFullAccess. AmazonSSMManagedInstanceCore (To remote into EC2 via Session Manager if needed). Click Next. Name the Role Auction-EC2-Role. Review and click Create role. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.2-database-storage/","title":"Database &amp; Storage","tags":[],"description":"","content":"Database and Storage Setup In this section, we will initialize:\nAmazon RDS: Relational Database (MySQL). Amazon ElastiCache: Redis cache. Amazon S3: Object storage (images, source code). These services will serve as the backend data storage for the application.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Auction system built in AWS Cloud infrastructure Auction web system built on Amazon Web Service cloud platform 1. Executive Summary The Auction system is designed by a FPTU student in Ho Chi Minh City and operates on the AWS Cloud platform. The platform utilizes AWS services to build an online auction marketplace with a user-friendly interface, easy to use and suitable for everyone.\n2. Problem Statement Current Problem Currently, auction systems have not reached many people due to difficulties in accessibility. This project was born to bring a transparent live auction platform that is friendly and accessible to everyone.\nSolution The platform uses AWS CloudFront and S3 Storage combined with ReactJS to provide the web interface, with EC2 servers handling all processing tasks on the Springboot platform, Amazon S3 for storing public and private data, and AWS RDS for database storage. Combined with Amazon Rekognition and Textract to extract information and verify user information to ensure fairness. With this platform, users can register new accounts, verify identities, and participate in exciting auctions on the platform.\nBenefits and Return on Investment (ROI) The project brings an online auction platform that is easily accessible to everyone. Estimated monthly cost is $59.37 USD (according to AWS Pricing Calculator). No additional development costs incurred.\n3. Solution Architecture The platform applies AWS architecture for data management. Public data is stored in public S3 buckets and displayed to users via CloudFront and S3 with ReactJS. All processing operations are performed on EC2 with the Springboot platform. Identity information is processed by Amazon Rekognition and Textract and then stored in private S3 buckets.\nAWS Services Used\nAWS VPC: Create a private virtual network environment. AWS Route 53: Route user traffic. AWS CloudFront: CDN helps accelerate page loading speed, reduce web access latency. AWS Load Balancing: Receive requests from the internet and route to EC2, stabilizing the application. Amazon EC2: Run springboot application to handle backend processing, communicate with database (RDS), cache queries (ElastiCache), call AI services (Rekognition, Textract) and process auctions. Amazon S3: Hosting Frontend: Store frontend source code (ReactJS, Tailwind) for CloudFront distribution. Data storage: 2 buckets (public/private) to store images uploaded by users for auctions and account verification. Amazon ElastiCache: Cache memory, helping store queries to reduce load for database and accelerate API response speed. Amazon RDS: Store main system data, placed in private subnet. Amazon Rekognition: AI image analysis service, performing Face Compare between selfie photos and ID photos to verify identity (eKYC). Amazon Textract: Text extraction service from documents. The system uses Textract to automatically read (OCR) and extract information from ID photos to automatically fill for users. Amazon SES: Backend uses this service to send account verification emails (OTP), auction winning notifications or other system notifications to users. Amazon CloudWatch: Service for monitoring and log management. Component Design\nData Ingestion: Data from users. Data Storage: Data stored in 2 S3 buckets (1 for public and 1 for private - accessed via presigned url) Data Processing: EC2 performs data processing. Web Interface: Amazon S3 stores ReactJS application. 4. Technical Implementation Implementation Stages The project includes the following stages:\nResearch and Architecture Design: Research and design AWS architecture, identify services to be used, design database. Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate and adjust. Development, Testing, Deployment: Program Springboot and ReactJS application, then test in local environment. Deployment on AWS Cloud Environment: Set up Gitlab CI, set up cloud environment and deploy. Technical Requirements\nJava 21 Springboot AWS SDK (S3, Rekognition, Textract, SES) MySQL RDS ReactJS/Vite/TypeScript/Tailwind Gitlab, Gitlab runner CI Postman, CloudWatch 5. Roadmap \u0026amp; Implementation Milestones Pre-internship (Month 0): 1 month for planning and evaluating the old station. Internship (Month 1–3): Month 1: Learn AWS and design architecture, design database, implement API construction. Month 2: Implement API construction, build interface. Month 3: Deploy on cloud environment, test, put into use. 6. Budget Estimate Costs can be viewed on AWS Pricing Calculator\nInfrastructure Costs\nAmazon Route53: $0.5/month (1 hosted zone) S3 Standard: $0.72/month (10 GB, 30000 request GET, 1000 request PUT, 5 GB Transfer out) Application Load Balancer: $18.80/month (data process 50GB) Amazon EC2 (t3.medium): $16.35/month (3yr, no upfront) Amazon ElastiCache (cache.t3.micro): $9.49/month (3yr, no upfront) Amazon RDS: $8.38/month Rekognition: $0.13/month (100 FaceCompare) Textract: $5/month (200 Pages document) Total: $59.37/month.\n7. Risk Assessment Risk Matrix\nNetwork Loss: High impact, low probability. Budget Overrun: Medium impact, low probability. Mitigation Strategy\nCost: AWS budget alerts, service optimization. Contingency Plan\nPeriodic backups in case of incidents. Use CloudFormation to restore cost-related configurations. 8. Expected Results Long-term Value: Can be reused for other projects in the future.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand Managed Relational Databases on AWS (Amazon RDS). Configure network security between Compute (EC2) and Database (RDS). Deploy a simple dynamic web application connecting to the database. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn RDS concepts (Multi-AZ, Read Replicas, Backups, Maintenance Window).\n- Practice: Create a MySQL RDS instance (Free Tier). 22/09/2025 22/09/2025 https://docs.aws.amazon.com/rds/ 3 - Configure Security Groups: Allow traffic from EC2 Security Group to RDS Security Group on port 3306.\n- Learn about RDS endpoints. 23/09/2025 23/09/2025 https://docs.aws.amazon.com/rds/ 4 - Install a simple LAMP stack (Linux, Apache, MySQL/MariaDB, PHP) on an EC2 instance.\n- Prepare a sample PHP connection script. 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: Connect the PHP application on EC2 to the RDS endpoint.\n- Troubleshoot connection issues (check VPC, Subnets, SG). 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about RDS Snapshots (Manual vs Automated).\n- Practice: Take a manual snapshot and restore it to a new DB instance. 26/09/2025 26/09/2025 https://docs.aws.amazon.com/rds/ Week 3 Achievements: Launched a fully managed MySQL database using Amazon RDS. Implemented best security practices by restricting database access only to the Web Server (EC2) via Security Group referencing. Successfully deployed a dynamic web application stack (LAMP) on EC2. Established connectivity between the Application Tier (EC2) and Data Tier (RDS). Performed database backup and recovery operations using RDS Snapshots. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.4-distribution/5.4.3-cloudfront/","title":"Amazon CloudFront","tags":[],"description":"","content":"Amazon CloudFront CloudFront helps distribute static content from S3 with low latency.\nSteps Access CloudFront Console -\u0026gt; Create distribution.\nIn Origin domain, select the Frontend S3 Bucket. In Origin access, select Legacy access identities or OAC (Origin Access Control) to restrict direct user access to S3. Select Create new OAC.\nViewer protocol policy: Redirect HTTP to HTTPS.\nAllowed HTTP methods: GET, HEAD, OPTIONS.\nWAF: Do not enable (to save costs). Alternate domain name (CNAME): Enter frontend domain (e.g., www.example.com).\nCustom SSL certificate: Select ACM certificate. Default root object: index.html. Click Create distribution.\nAfter creation, remember to update the Bucket Policy of S3 to allow OAC access (Console will suggest copying the policy).\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.2-database-storage/5.2.3-s3/","title":"Amazon S3","tags":[],"description":"","content":"Initialize Amazon S3 Buckets We need 3 buckets for different purposes.\n1. Frontend Bucket Used for static web hosting (ReactJS).\nCreate a bucket with a unique name. Uncheck Block all public access (as the web needs to be public). Enable Static website hosting. 2. Public Storage Bucket Stores auction product images (public read).\nCreate a bucket. Uncheck Block all public access. Configure Bucket Policy to allow s3:GetObject. 3. Private Storage Bucket Stores identity documents for account verification (private).\nCreate a bucket. Keep Block all public access checked. (Optional) Configure Server-side encryption. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.3-compute/","title":"Compute","tags":[],"description":"","content":"Initialize Amazon EC2 Amazon EC2 (Elastic Compute Cloud) will be where the backend application (Spring Boot) runs.\nContent EC2 Instance: Virtual server running the application. Security Group: Firewall controlling access. Elastic IP: Static IP address (optional, necessary if not using a Load Balancer or need a fixed outbound IP). "},{"uri":"https://quangtruong278.github.io/FCJBlog/4-eventparticipated/4.2-event2/","title":"Event Report: AI/ML/GenAI on AWS","tags":[],"description":"","content":"Summary Report: “AI/ML/GenAI on AWS Workshop” Event Objectives Gain an overview of the AI/ML landscape in Vietnam. Understand the end-to-end Machine Learning capabilities of Amazon SageMaker. Deep dive into Generative AI with Amazon Bedrock (Foundation Models, Agents, Guardrails). Learn practical implementation through live demos of SageMaker Studio and Bedrock Chatbots. Speakers AWS Solutions Architects Team (Vietnam) Key Highlights AWS AI/ML Services Overview (Amazon SageMaker) End-to-end Platform: SageMaker provides a unified interface for the entire ML lifecycle, from data preparation and labeling to model building, training, tuning, and deployment. MLOps Integration: Emphasized the importance of MLOps capabilities built into SageMaker to streamline operations and ensure model reproducibility. SageMaker Studio: A live walkthrough demonstrated how to manage ML workflows visually, making it accessible for both data scientists and developers. Generative AI with Amazon Bedrock Foundation Models (FMs) Choice: Comparison of different models available on Bedrock (Claude, Llama, Titan) and guidance on how to select the right model for specific use cases (speed vs. accuracy vs. cost). Prompt Engineering: Techniques to improve model output, including Chain-of-Thought reasoning and Few-shot learning. RAG (Retrieval-Augmented Generation): Explained the architecture for connecting FMs to internal knowledge bases to reduce hallucinations and provide accurate, domain-specific answers. Agents \u0026amp; Guardrails: How to use Agents for multi-step tasks and Guardrails to ensure safety and content filtering. Key Takeaways Simplification of ML Workflow SageMaker drastically reduces the operational overhead of managing ML infrastructure, allowing teams to focus on model logic rather than server maintenance. Democratization of GenAI Amazon Bedrock offers a serverless experience to access top-tier Foundation Models via a single API, removing the need to manage heavy GPU infrastructure. RAG is Essential: For enterprise applications, RAG is the key pattern to make GenAI useful and reliable by grounding it in company data. Applying to Work Explore SageMaker Canvas: Try using SageMaker\u0026rsquo;s low-code features for rapid prototyping of tabular data models. Build a RAG Prototype: Use Amazon Bedrock to build a simple Q\u0026amp;A bot connected to our internal documentation (e.g., internship guides or project specs). Practice Prompt Engineering: Apply \u0026ldquo;Chain-of-Thought\u0026rdquo; techniques to improve the quality of code generation or documentation tasks. Event Experience The \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was an intense and highly practical session.\nHands-on Focus Unlike high-level seminars, this workshop focused heavily on Live Demos. Seeing a GenAI chatbot being built from scratch using Bedrock Agents gave me a concrete understanding of how the components (FMs, Knowledge Bases, Action Groups) fit together.\nComprehensive Toolset I learned that AWS has a tool for every stage of the AI journey. Whether it\u0026rsquo;s traditional ML (SageMaker) or cutting-edge GenAI (Bedrock), the ecosystem is highly integrated.\nNetworking The morning \u0026ldquo;Ice-breaker\u0026rdquo; and networking session allowed me to connect with other developers and students interested in AI, giving me a broader perspective on how AI is being adopted in the Vietnam market.\nSome event photos Add your event photos here\nOverall, the workshop demystified the buzzwords around GenAI and provided a clear, technical path to building AI applications on AWS.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, I list and describe the technical events and workshops I participated in during my internship. These events provided valuable opportunities to learn from industry experts, gain hands-on experience with new AWS services, and network with the tech community.\nDuring my internship, I actively participated in 5 key events. Each one was a memorable experience that provided new, interesting, and useful knowledge, helping me shape my technical mindset and career path.\nEvent 1 Event Name: AWS GenAI \u0026amp; Data Revolution\nDate \u0026amp; Time: 13:00, October 24, 2025\nLocation: AWS Vietnam Office, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: AWS Vietnam Office, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: AWS Vietnam Office, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: AWS Vietnam Office, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: Agentic AI: From Architecture to Implementation\nDate \u0026amp; Time: 09:00, December 02, 2025 (Estimated)\nLocation: AWS Vietnam Office, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Implement High Availability (HA) and Scalability for web applications. Master the configuration of Application Load Balancer (ALB) and Auto Scaling Group (ASG). Perform stress testing to verify automatic scaling. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Elastic Load Balancing (ELB) types: ALB vs NLB.\n- Create Target Groups and register targets.\n- Practice: Create an Application Load Balancer. 29/09/2025 29/09/2025 https://docs.aws.amazon.com/elasticloadbalancing/ 3 - Understand Amazon Machine Image (AMI).\n- Practice: Create a custom AMI from the Web Server (EC2) created in Week 3 (with LAMP stack installed). 30/09/2025 30/09/2025 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html 4 - Learn about Launch Templates and Auto Scaling Groups (ASG).\n- Configure Scaling Policies (e.g., Target Tracking Scaling: CPU \u0026gt; 50%). 01/10/2025 01/10/2025 https://docs.aws.amazon.com/autoscaling/ 5 - Integration: Attach the Auto Scaling Group to the Application Load Balancer.\n- Ensure the Health Check is configured correctly. 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Stress Test: Simulate high traffic to the ALB DNS.\n- Observe EC2 instances scaling out (adding new instances) automatically.\n- Verify the \u0026ldquo;Self-healing\u0026rdquo; capability by terminating an instance manually. 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Successfully created a custom AMI (Amazon Machine Image) for rapid application deployment. Designed and built a highly available architecture using Application Load Balancer (ALB) to distribute traffic. Configured Auto Scaling Group (ASG) to ensure the application can handle varying load levels. Verified the system\u0026rsquo;s elasticity: Scale-out: Automatically added instances when CPU load increased. High Availability: Automatically replaced unhealthy/terminated instances. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.4-distribution/5.4.4-route53/","title":"Amazon Route 53","tags":[],"description":"","content":"Amazon Route 53 Configure DNS to point domain to CloudFront (Frontend) and ALB (Backend).\nSteps Access Route 53 -\u0026gt; Hosted zones. Select your domain. Route53 will provide 4 NS records, configure them at your domain registrar to manage the domain in Route53. This process takes a few minutes. 1. Point to Backend (ALB) Create record. Record name: api (e.g., api.example.com). Record type: A. Enable Alias. Route traffic to: Alias to Application and Classic Load Balancer. Select Region and your ALB. 2. Point to Frontend (CloudFront) Create record. Record name: www or leave blank (root domain). Record type: A. Enable Alias. Route traffic to: Alias to CloudFront distribution. Select CloudFront distribution. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.4-distribution/","title":"Distribution","tags":[],"description":"","content":"Content Distribution This section helps deliver the application to end users securely and with high performance.\nContent Certificate (ACM): Issue free SSL/TLS certificates. Application Load Balancer (ALB): Load balance for Backend (Spring Boot). CloudFront: CDN to distribute Frontend (ReactJS) and Static files (S3). Route 53: Manage Domain DNS. "},{"uri":"https://quangtruong278.github.io/FCJBlog/4-eventparticipated/4.3-event3/","title":"Event Report: DevOps on AWS","tags":[],"description":"","content":"Summary Report: “DevOps on AWS Workshop” Event Objectives Understand the DevOps culture, principles, and key performance metrics (DORA). Master the AWS Developer Tools suite to build automated CI/CD pipelines. Learn Infrastructure as Code (IaC) using AWS CloudFormation and CDK. Explore Container services (ECS, EKS, App Runner) and Observability tools (CloudWatch, X-Ray). Speakers AWS Solutions Architects Team (Vietnam) Key Highlights DevOps Mindset \u0026amp; CI/CD Services Metrics Matter: The session started by emphasizing DORA metrics (Deployment Frequency, Lead Time, etc.) to measure DevOps success. Pipeline Automation: Detailed walkthrough of the full pipeline: Source: CodeCommit \u0026amp; Git strategies (Trunk-based vs. GitFlow). Build \u0026amp; Test: Using CodeBuild for compiling and running unit tests. Deploy: Strategies like Blue/Green and Canary using CodeDeploy to minimize downtime. Orchestrate: Binding everything together with CodePipeline. Infrastructure as Code (IaC) CloudFormation vs. CDK: A clear comparison between defining infrastructure via JSON/YAML templates (CloudFormation) versus using familiar programming languages like TypeScript/Python (CDK). Drift Detection: How to detect if the actual infrastructure has deviated from the code definition. Container Services Choosing the Right Tool: Guidance on when to use Amazon ECS (simpler, AWS-native) vs. Amazon EKS (Kubernetes standard) vs. App Runner (fully managed/simplest). Registry: Using Amazon ECR for secure image storage and vulnerability scanning. Observability Full-stack Visibility: It’s not just about logs. It’s about correlating Metrics, Logs, and Traces (using AWS X-Ray) to debug distributed microservices effectively. Key Takeaways Automation is King Manual deployments are error-prone. The goal is to automate everything from infrastructure provisioning (IaC) to code deployment (CI/CD). Immutable Infrastructure Through the Container session, I learned the value of immutable infrastructure—packaging the app and its dependencies into a Docker container ensures it runs the same way everywhere. Shift Left Testing and security scanning should happen early in the pipeline (in CodeBuild/ECR), not after deployment. Applying to Work Refactor Pipeline: I will attempt to implement a \u0026ldquo;Blue/Green\u0026rdquo; deployment strategy for my current internship project using CodeDeploy to ensure zero downtime. Try AWS CDK: Instead of clicking through the console, I will write a CDK script to provision the VPC and EC2 for the next task. Enable X-Ray: I plan to instrument my application to send traces to AWS X-Ray to visualize service maps and latency bottlenecks. Event Experience The \u0026ldquo;DevOps on AWS\u0026rdquo; full-day workshop was comprehensive and connected many dots for me.\nFrom Theory to Practice The transition from explaining the DevOps Mindset in the morning to actually configuring a CI/CD Pipeline helped me understand why we use these tools, not just how.\nThe \u0026ldquo;Container\u0026rdquo; Clarity I used to be confused between ECS and EKS. The breakdown in the afternoon session clarifying that ECS is for \u0026ldquo;AWS-opinionated\u0026rdquo; orchestration and EKS is for \u0026ldquo;Kubernetes-standard\u0026rdquo; orchestration was very helpful.\nInteractive Demos The demo on Full-stack observability using CloudWatch and X-Ray was an eye-opener. Seeing a request traced from the load balancer down to the database query showed the power of modern monitoring.\nSome event photos Overall, this event provided the essential toolkit for a modern Cloud Engineer: CI/CD, IaC, Containers, and Monitoring.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/4-eventparticipated/4.4-event4/","title":"Event Report: AWS Security Pillar","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected Security Pillar” Event Objectives Master the core principles of the Security Pillar: Least Privilege, Zero Trust, and Defense in Depth. Understand Modern IAM architecture and how to move away from long-term credentials. Learn to implement continuous detection and monitoring layers (GuardDuty, Security Hub). Deep dive into Infrastructure/Data protection and Incident Response automation. Speakers AWS Solutions Architects Team (Security Specialty) Key Highlights 1. Identity \u0026amp; Access Management (IAM) Identity is the New Perimeter: The session emphasized moving from network-based security to identity-based security. Best Practices: Avoid long-term IAM User credentials. Instead, use IAM Identity Center for SSO and temporary credentials via Roles. Access Control: Using SCPs (Service Control Policies) for multi-account governance and Access Analyzer to validate policies. 2. Detection \u0026amp; Monitoring Centralized Visibility: Integrating CloudTrail (for API auditing) and VPC Flow Logs with AWS Security Hub to have a single pane of glass for security posture. Detection-as-Code: Treating security rules and alerts as code to ensure consistency across environments. 3. Infrastructure \u0026amp; Data Protection Defense in Depth: Layering security controls: Network: VPC Segmentation, WAF, Shield, and Network Firewall. Data: Encryption at-rest and in-transit using AWS KMS. Secrets Management: Using AWS Secrets Manager to automatically rotate database credentials instead of hardcoding them in the app. 4. Incident Response (IR) The \u0026ldquo;Playbook\u0026rdquo; Concept: Defined specific steps for common scenarios like a compromised IAM key or S3 public exposure. Automation: Using Lambda or Step Functions to automatically isolate a compromised EC2 instance or revoke user permissions immediately upon detection. Key Takeaways Zero Trust Architecture Never trust, always verify. Every request, whether internal or external, must be authenticated and authorized. Automate Security Human speed is not enough for cyber threats. Security responses (like isolating a server) must be automated using EventBridge and Lambda. Shared Responsibility Re-learned the model: AWS secures the \u0026ldquo;Cloud\u0026rdquo; (hardware, global infra), while the customer secures \u0026ldquo;in the Cloud\u0026rdquo; (data, configuration, patching). Applying to Work Audit IAM Policies: Use IAM Access Analyzer to check for overly permissive policies in my current project. Implement Secrets Manager: Remove all hardcoded API keys from my code and replace them with calls to AWS Secrets Manager. Enable GuardDuty: Turn on Amazon GuardDuty in the training account to detect potential threats. Event Experience The \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop provided a structured framework for thinking about security.\nStructured Learning Breaking down security into 5 distinct pillars (IAM, Detection, Infra, Data, IR) made a vast and complex topic manageable and easy to digest.\nPractical \u0026ldquo;Mini Demo\u0026rdquo; The live demo on Validating IAM Policy and simulating access was very helpful. It showed how to verify permissions before deploying, preventing \u0026ldquo;Access Denied\u0026rdquo; errors in production.\nReal-world Relevance The discussion on \u0026ldquo;Top threats in the Vietnam cloud environment\u0026rdquo; and common pitfalls gave me a realistic view of the security landscape I will face in my career.\nSome event photos Add your event photos here\nOverall, this event shifted my mindset from \u0026ldquo;Security is a blocker\u0026rdquo; to \u0026ldquo;Security is an enabler\u0026rdquo; for modern application development.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master Identity and Access Management (IAM) concepts. Implement the \u0026ldquo;Principle of Least Privilege\u0026rdquo; for users and services. Secure AWS resources using IAM Roles instead of long-term credentials. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn IAM core components: Users, Groups, Policies (JSON structure).\n- Practice: Create IAM Groups (Dev, Admin) and assign managed policies. 06/10/2025 06/10/2025 https://docs.aws.amazon.com/iam/ 3 - Learn about IAM Roles and Service-Linked Roles.\n- Scenario: Create an IAM Role allowing EC2 to access S3 buckets (Read/Write) without Access Keys. 07/10/2025 07/10/2025 https://docs.aws.amazon.com/iam/ 4 - Practice: Attach the created IAM Role to an running EC2 instance.\n- SSH into EC2 and verify S3 access via AWS CLI (aws s3 ls). 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Improve Account Security:\n- Enable MFA (Multi-Factor Authentication) for the Root user and IAM users.\n- Configure Password Policy (length, complexity). 09/10/2025 09/10/2025 https://docs.aws.amazon.com/iam/ 6 - Review AWS Shared Responsibility Model.\n- Explore AWS CloudTrail to audit API calls (who did what, when). 10/10/2025 10/10/2025 https://aws.amazon.com/compliance/shared-responsibility-model/ Week 5 Achievements: Deeply understood the AWS Shared Responsibility Model and IAM logic. Implemented strict permission boundaries using IAM Policies and Groups. Successfully replaced hard-coded credentials (Access/Secret Keys) with secure IAM Roles for EC2 service access. Enhanced account security by enforcing MFA and strong password policies. Verified secure cross-service communication (EC2 talking to S3) without exposing secrets. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.5-deploy/","title":"CI/CD","tags":[],"description":"","content":"Continuous Integration and Deployment (CI/CD) Use GitLab CI to automate the process of building and deploying the application to AWS.\nContent SSH into EC2 instance to install necessary applications. GitLab Runner: Configure runner on EC2 (or use Shared Runner). Pipeline: Define .gitlab-ci.yml file. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/","title":"System Deployment","tags":[],"description":"","content":"Deploy Auction System on AWS Welcome to the workshop on deploying an online auction system on the AWS platform. In this workshop, we will build each component of the system together based on the proposed architecture.\nObjectives Complete the installation and configuration of necessary AWS services to operate the Auction System.\nArchitecture We will follow this architecture:\nSteps Preparation: Setup VPC, IAM Role. Database \u0026amp; Storage: Configure RDS, ElastiCache, S3. Compute: Install and configure EC2. Distribution: Configure Load Balancer, CloudFront, Route 53. CI/CD: Setup automated deployment process with GitLab CI. Clean up: Delete resources after completion. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand Amazon Simple Email Service (SES) and its use cases (Transactional vs Marketing). Verify Identities (Email/Domain) and understand Sandbox restrictions. Programmatically send emails using AWS SDK (Boto3 with Python) on EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Amazon SES.\n- Practice: Verify an Email Identity (Sender and Receiver) in the SES Console.\n- Understand the concept of \u0026ldquo;SES Sandbox\u0026rdquo; mode. 13/10/2025 13/10/2025 https://docs.aws.amazon.com/ses/ 3 - Configure SMTP Credentials for SES.\n- Learn how to request production access (moving out of Sandbox). 14/10/2025 14/10/2025 https://docs.aws.amazon.com/ses/ 4 - Coding: Write a simple Python script using boto3 library to send a test email via SES API.\n- Handle exceptions (e.g., MessageRejected). 15/10/2025 15/10/2025 https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ses.html 5 - Integration: Deploy the Python script to an EC2 instance.\n- Configure IAM Role for EC2 to allow ses:SendEmail permission (instead of using hardcoded SMTP credentials). 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Test: Run the script on EC2 to trigger an automated email notification.\n- Monitor sending statistics in the SES Console (Deliverability, Bounce rate). 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Successfully configured Amazon SES environment and verified email identities. Understood the security best practices by using IAM Roles for EC2 to authorize email sending. Developed a Python automation script using AWS SDK (Boto3) to interact with AWS services. Integrated email notification capability into the infrastructure, enabling automated alerts. "},{"uri":"https://quangtruong278.github.io/FCJBlog/5-workshop/5.6-cleanup/","title":"Clean Up","tags":[],"description":"","content":"Clean Up Resources To avoid unwanted costs after completing the workshop, please delete resources in the following order:\nDeletion Order EC2: Terminate instances. RDS \u0026amp; ElastiCache: Delete database and cache cluster. Delete Subnet Groups and Snapshots. Load Balancer \u0026amp; Target Group: Delete ALB first, then Target Group. CloudFront: Disable distribution, wait for deployment to finish, then Delete. S3: Empty bucket (delete all objects) then Delete bucket. NAT Gateway \u0026amp; Elastic IP: Delete NAT Gateway -\u0026gt; Release Elastic IP. VPC: Delete VPC (this will automatically delete Subnets, Internet Gateway, Route Tables, and related Security Groups). Note: Check the Billing Dashboard the next day to ensure there are no incurring costs.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/4-eventparticipated/4.5-event5/","title":"Event Report: Agentic AI Journey","tags":[],"description":"","content":"Summary Report: “Agentic AI: From High-level Architecture to Hands-on” Event Objectives Understand the core concepts of Agentic AI and AWS Bedrock Agents. Explore real-world use cases for building Agentic Workflows. Deep dive into Agentic Orchestration and Context Optimization (Level 300 technical depth). Hands-on experience: Building an agent using CloudThinker and AWS Bedrock. Speakers Nguyen Gia Hung – Head of Solutions Architect Kien Nguyen – Solutions Architect (AWS) Viet Pham – Founder \u0026amp; CEO Thang Ton – Co-founder \u0026amp; COO (CloudThinker) Henry Bui – Head of Engineering (CloudThinker) Kha Van – Technical Lead/Facilitator Key Highlights AWS Bedrock Agent Core Concept: Moving beyond passive \u0026ldquo;Chatbots\u0026rdquo; to active \u0026ldquo;Agents\u0026rdquo; that can execute multi-step tasks. Components: Explained the anatomy of an Agent: Foundation Model (Brain) + Action Groups (Tools/APIs) + Knowledge Bases (RAG). Functionality: How Bedrock Agents break down a complex user prompt into a chain of logical steps (Chain-of-Thought) and execute them via Lambda functions. Real-world Application \u0026amp; CloudThinker Use Cases: Creating workflows where AI autonomously handles business processes (e.g., booking, order processing, data analysis) without human intervention. Orchestration (L300 Deep Dive): Henry Bui shared advanced techniques on managing complex agent interactions. The focus was on Context Optimization—ensuring the AI retains relevant information across long conversations without hitting token limits or losing focus. CloudThinker Platform: Introduction to a specialized framework designed to simplify the orchestration of complex agentic workflows on top of AWS. Hands-on Workshop: CloudThinker Hack Practical Lab: We didn\u0026rsquo;t just listen; we built. Guided by Kha Van, we set up a working environment to deploy a functional agent. Integration: Connected the agent to external tools and observed how it reasoned through tasks in real-time. Key Takeaways The Shift to \u0026ldquo;Agency\u0026rdquo; The future of AI is not just generating text, but taking action. \u0026ldquo;Agentic AI\u0026rdquo; is the next frontier where models become decision-makers. Context is King In complex workflows, managing \u0026ldquo;State\u0026rdquo; and \u0026ldquo;Context\u0026rdquo; is the hardest challenge. Optimization techniques (like summarizing context or selective retention) are critical for production-grade agents. Orchestration Complexity Building one agent is easy; making multiple agents work together requires a robust orchestration layer (which tools like CloudThinker aim to solve). Applying to Work Experiment with Action Groups: I will try adding a simple \u0026ldquo;Action Group\u0026rdquo; to my Bedrock test project (e.g., a Lambda function that queries a database) to turn my chatbot into a basic agent. Study Context Management: Research more on how to optimize prompt context for long-running tasks, as emphasized in the L300 session. Blue/Green Testing for Agents: Apply the testing concepts learned to evaluate if an agent is performing actions correctly before full deployment. Event Experience The \u0026ldquo;Agentic AI\u0026rdquo; event was a perfect blend of theory and practice.\nTechnical Depth (L300) I particularly appreciated the L300 session by Henry Bui. Often events stay high-level, but this session went deep into the engineering challenges of Context Optimization, which gave me valuable architectural insights.\nThe \u0026ldquo;Hack\u0026rdquo; Component The hands-on session at the end was energetic and rewarding. Seeing the agent I configured actually execute a task (instead of just replying with text) was a \u0026ldquo;lightbulb moment\u0026rdquo; regarding the power of the Bedrock ecosystem.\nNetworking The lunch buffet and networking time allowed me to discuss with the CloudThinker team about the challenges of latency in agentic workflows, providing practical tips for my own projects.\nSome event photos Add your event photos here\nOverall, this event bridged the gap between the \u0026ldquo;hype\u0026rdquo; of AI Agents and the \u0026ldquo;how-to\u0026rdquo; of building them on AWS.\n"},{"uri":"https://quangtruong278.github.io/FCJBlog/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey (FCJ) from September 8th, 2025 to November 28th, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI actively participated in the DevOps \u0026amp; Cloud Computing Roadmap, through which I improved my skills in AWS services (EC2, S3, VPC), Infrastructure as Code (Terraform), CI/CD pipelines, and Containerization.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency. However, I recognize that there is still significant room for improvement in my professional discipline and soft skills.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria (Self-rating: 6.5/10):\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ☐ ✅ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Discipline: Need to strengthen self-discipline and strictly comply with the schedules and regulations of the organization (e.g., punctuality, report deadlines). Problem-solving: Need to improve critical thinking when facing technical errors, learning to troubleshoot independently before seeking help. Communication: Enhance communication skills in professional contexts, specifically in presenting technical ideas clearly and handling feedback effectively. Focus: Need to maintain higher concentration levels during working hours to improve productivity. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Transition from manual configuration (\u0026ldquo;ClickOps\u0026rdquo;) to Infrastructure as Code (IaC). Learn Terraform basics: Providers, Resources, Variables, and State. Automate the provisioning of VPC and EC2 resources using Terraform. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to IaC concepts.\n- Install Terraform and configure AWS credentials locally.\n- Learn HCL (HashiCorp Configuration Language) syntax. 20/10/2025 20/10/2025 https://developer.hashicorp.com/terraform/intro 3 - Coding: Write a vpc.tf file to create a VPC, Subnet, Internet Gateway, and Route Table.\n- Learn about Terraform Providers (aws). 21/10/2025 21/10/2025 https://registry.terraform.io/providers/hashicorp/aws/latest/docs 4 - Coding: Write a main.tf to launch an EC2 instance inside the created VPC.\n- Use variables.tf to parameterize values (Region, AMI ID, Instance Type). 22/10/2025 22/10/2025 https://developer.hashicorp.com/terraform/language 5 - Understand Terraform Workflow: init, plan, apply, destroy.\n- Learn about terraform.tfstate and how to manage state files. 23/10/2025 23/10/2025 https://developer.hashicorp.com/terraform/cli/commands 6 - Mini-Project: Re-create the Week 2 architecture (VPC + EC2) entirely using Terraform code.\n- Verify the deployment via AWS Console and then destroy resources with one command. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Successfully transitioned from manual Console actions to Code-defined infrastructure using Terraform. Mastered the core Terraform workflow (init -\u0026gt; plan -\u0026gt; apply -\u0026gt; destroy). Created reusable infrastructure code with Variables. Deployed a fully functional VPC and EC2 instance environment in minutes using a single command. Understood the importance of State Management in IaC. "},{"uri":"https://quangtruong278.github.io/FCJBlog/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Below are my honest reflections after 12 weeks of internship. These opinions are based on my personal experience transitioning from academic knowledge to real-world corporate practice.\nOverall Evaluation 1. Working Environment The environment at FCJ is highly technical and fast-paced. Instead of \u0026ldquo;hand-holding,\u0026rdquo; this environment pushes interns to be independent and proactive in their research. Initially, I felt a bit overwhelmed, but this professional pressure helped me adapt quickly to industry standards. This is a place for those who truly want to do \u0026ldquo;real work\u0026rdquo; rather than just observe.\n2. Support from Mentor / Team Admin The Mentors are quite strict but fair. When I encountered errors, instead of providing the solution immediately, the Mentor guided me on how to read logs and apply Root Cause Analysis. Although this approach takes time initially, it is extremely effective in honing a troubleshooting mindset. The Admin Team is very quick to offer support.\n3. Relevance of Work to Academic Major The gap between university theory and workplace reality is quite significant. While the university provides foundational programming thinking, this internship filled the gaps regarding Infrastructure, Networking, and Deployment Strategies. The tasks related to AWS Services and DevOps processes were entirely new and highly relevant to a Cloud Engineer career.\n4. Learning \u0026amp; Skill Development Opportunities Beyond hard skills (AWS, Docker, IaC), the most valuable lesson I received was \u0026ldquo;System Thinking.\u0026rdquo; I learned to view software not just as lines of code, but as a complete lifecycle from development and operation to monitoring. The weekly technical workshops (GenAI, Security) were high-quality value-adds that broadened my perspective.\n5. Company Culture \u0026amp; Team Spirit The culture of \u0026ldquo;Knowledge Sharing\u0026rdquo; here is very strong. No question is considered \u0026ldquo;bad\u0026rdquo; if I have genuinely tried to research it beforehand. I appreciate the transparency in technical discussions, where even interns\u0026rsquo; opinions are listened to during architecture reviews.\n6. Internship Policies / Benefits The internship roadmap is structured methodically week-by-week. The internship is fair, with flexible working hours and fully equipped facilities.\nAdditional Questions What did you find most satisfying during your internship? The moment I successfully built a fully automated CI/CD pipeline, deploying code to EC2 without manual intervention. That was the tangible result connecting all the disjointed concepts I had learned.\nWhat do you think the company should improve for future interns? I suggest having more in-depth Code Review sessions specifically for interns. Sometimes my code \u0026ldquo;works,\u0026rdquo; but I am not sure if it follows best practices or if it is optimized for cost/security. Receiving detailed feedback on code quality would help us improve faster.\nIf recommending to a friend, would you suggest they intern here? Why or why not? Definitely yes. However, I would clarify beforehand: I only recommend FCJ to friends who are truly proactive and highly disciplined. This is not a place for passive learners, but it is an excellent launchpad for those who are serious about becoming a Cloud Engineer.\nSuggestions \u0026amp; Expectations Suggestion: It would be great to have a \u0026ldquo;Mock Interview\u0026rdquo; session at the end of the internship to help us mentally prepare for applying for official jobs. Expectation: I hope to maintain a connection with the FCJ community. Closing: Thank you to the FCJ team for a challenging but very rewarding 3 months. The strict standards here have helped me become a better version of myself. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Understand DevOps culture and the concept of Continuous Integration/Continuous Deployment (CI/CD). Familiarize with AWS Developer Tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline). Build a fully automated pipeline to deploy code changes to EC2. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to CI/CD pipelines.\n- Create a Git repository in AWS CodeCommit (or connect a GitHub repo).\n- Push the sample web app code to the repository. 27/10/2025 27/10/2025 https://docs.aws.amazon.com/codecommit/ 3 - Configure AWS CodeBuild:\n- Create a buildspec.yml file to define build commands (e.g., install dependencies, run tests).\n- Run a test build manually. 28/10/2025 28/10/2025 https://docs.aws.amazon.com/codebuild/ 4 - Configure AWS CodeDeploy:\n- Create an appspec.yml file to define deployment instructions.\n- Install CodeDeploy Agent on the target EC2 instance. 29/10/2025 29/10/2025 https://docs.aws.amazon.com/codedeploy/ 5 - Integration: Create a Pipeline in AWS CodePipeline.\n- Link the stages: Source (CodeCommit) -\u0026gt; Build (CodeBuild) -\u0026gt; Deploy (CodeDeploy). 30/10/2025 30/10/2025 https://docs.aws.amazon.com/codepipeline/ 6 - Test: Push a code change (e.g., change website text) to the repository.\n- Watch the pipeline trigger automatically and update the server without manual intervention. 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Gained practical understanding of the CI/CD workflow on AWS. Successfully set up a source control repository using AWS CodeCommit. Automate the build process using CodeBuild and deployment using CodeDeploy. Constructed a complete CI/CD pipeline using AWS CodePipeline. significantly reduced deployment time and human error by automating the release process. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Understand Containerization technology and Docker. Learn how to write a Dockerfile to containerize an application. Use Amazon Elastic Container Registry (ECR) to store and manage Docker images. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Containers vs Virtual Machines.\n- Install Docker Engine on an EC2 instance (or local machine).\n- Learn basic Docker commands (run, ps, images, stop). 03/11/2025 03/11/2025 https://docs.docker.com/get-started/ 3 - Practice: Write a Dockerfile for a simple Python/Node.js web application.\n- Build the Docker image locally. 04/11/2025 04/11/2025 https://docs.docker.com/engine/reference/builder/ 4 - Introduction to Amazon ECR.\n- Create a private repository in Amazon ECR.\n- Configure AWS CLI to authenticate Docker with ECR. 05/11/2025 05/11/2025 https://docs.aws.amazon.com/AmazonECR/ 5 - Practice: Tag the local Docker image and push it to the Amazon ECR repository.\n- Verify the image in the AWS Console. 06/11/2025 06/11/2025 https://docs.aws.amazon.com/AmazonECR/ 6 - Deployment: Launch a new EC2 instance.\n- Pull the Docker image from ECR and run the container.\n- Verify the application is accessible. 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood the benefits of containerization over traditional virtualization. Successfully containerized a legacy application using Docker (wrote optimized Dockerfile). Mastered the workflow of building, tagging, and pushing images to a registry. Used Amazon ECR as a secure, private Docker container registry. Deployed an application from a container image, ensuring consistency across environments. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Explore Serverless computing and Event-driven architecture. Build a RESTful API using Amazon API Gateway and AWS Lambda. Integrate Serverless functions with other AWS services (like SES from Week 6). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Serverless: AWS Lambda concepts (Triggers, Runtimes, Layers).\n- Practice: Create a simple \u0026ldquo;Hello World\u0026rdquo; Lambda function (Python/Node.js). 10/11/2025 10/11/2025 https://docs.aws.amazon.com/lambda/ 3 - Test Lambda with custom test events in the Console.\n- Learn about IAM Roles for Lambda (Execution Role). 11/11/2025 11/11/2025 https://docs.aws.amazon.com/lambda/ 4 - Introduction to Amazon API Gateway.\n- Practice: Create a REST API.\n- Create Resources and Methods (GET/POST). 12/11/2025 12/11/2025 https://docs.aws.amazon.com/apigateway/ 5 - Integration: Connect API Gateway to the Lambda function (Lambda Proxy Integration).\n- Deploy the API to a Stage (dev/prod). 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Advanced Practice: Migrate the email sending script (Week 6) to Lambda.\n- Trigger the Lambda function via an API call to send emails automatically. 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Successfully deployed code without provisioning or managing servers (Serverless). Created a scalable, public-facing API endpoint using Amazon API Gateway. Combined Serverless (Lambda) with SES to create an event-driven email notification system. Understood the cost benefits of the \u0026ldquo;Pay-as-you-go\u0026rdquo; model in Serverless. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Implement observability for the AWS infrastructure. Configure CloudWatch Alarms to proactively detect issues. Centralize logs and visualize metrics on Dashboards. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon CloudWatch concepts: Namespaces, Metrics, Dimensions.\n- Check standard EC2 metrics (CPU Utilization, Status Checks, Network In/Out). 17/11/2025 17/11/2025 https://docs.aws.amazon.com/cloudwatch/ 3 - Practice: Create a CloudWatch Alarm.\n- Condition: Send an email notification (via SNS topic) if CPU Utilization \u0026gt; 80% for 5 minutes. 18/11/2025 18/11/2025 https://docs.aws.amazon.com/cloudwatch/ 4 - Learn about CloudWatch Logs.\n- Install and configure the Unified CloudWatch Agent on EC2 to push OS logs (syslog/messages) to CloudWatch. 19/11/2025 19/11/2025 https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html 5 - Configure Custom Metrics: Monitor Memory Usage (RAM) and Disk Space (which are not standard metrics). 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Dashboarding: Create a centralized CloudWatch Dashboard.\n- Add widgets for EC2 CPU, Memory, API Gateway calls, and Lambda errors. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Established proactive monitoring for critical infrastructure components. Configured automated alerts (Alarms + SNS) to reduce response time to incidents. Gained visibility into internal system performance (Memory/Disk) using CloudWatch Agent. Created a professional Operations Dashboard to visualize system health in real-time. "},{"uri":"https://quangtruong278.github.io/FCJBlog/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Consolidate all learned knowledge and projects into a final comprehensive report. Review and optimize the created resources. Clean up the AWS environment to prevent unexpected billing (Cost Management). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review: Audit all active resources in all regions (EC2, RDS, NAT Gateways, Elastic IPs, ALBs).\n- Identify resources that are no longer needed. 24/11/2025 24/11/2025 AWS Billing Dashboard 3 - Cleanup: Terminate EC2 instances, delete RDS snapshots, release Elastic IPs, and delete NAT Gateways to stop billing.\n- Leave only free-tier eligible resources if necessary. 25/11/2025 25/11/2025 AWS Billing Dashboard 4 - Documentation: Finalize the \u0026ldquo;Final Internship Report\u0026rdquo; (Hugo project).\n- Compile all architectural diagrams drawn over the 12 weeks. 26/11/2025 26/11/2025 5 - Prepare presentation slides for the internship defense/summary meeting.\n- Rehearse the demo of key projects (CI/CD Pipeline, Serverless API). 27/11/2025 27/11/2025 6 - Self-evaluation of the internship period.\n- Submit the final worklog and report to the mentor/supervisor.\n- Official completion of the 12-week roadmap. 28/11/2025 28/11/2025 Week 12 Achievements: Successfully completed the comprehensive 12-week DevOps/AWS internship roadmap. Delivered a high-quality Final Report documenting all technical implementations. Conducted a thorough cleanup of the AWS environment, ensuring zero unexpected costs. Consolidated knowledge across Networking, Compute, Database, Security, DevOps, and Serverless domains. Ready for the final presentation and future career steps as a Cloud/DevOps Engineer. "},{"uri":"https://quangtruong278.github.io/FCJBlog/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://quangtruong278.github.io/FCJBlog/tags/","title":"Tags","tags":[],"description":"","content":""}]